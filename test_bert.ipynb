{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "* Tenir en compte la succession de phrases au sein d'un même commentaires (peut-être avec l'architecture type A / B) pour prendre en compte les coréférences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.nn as nn\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertModel, PreTrainedBertModel, BertConfig\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, skip_rows=0):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding='utf-8') as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\")\n",
    "            lines = []\n",
    "            row = 0\n",
    "            for line in reader:\n",
    "                if row >= skip_rows:\n",
    "                    lines.append(line) \n",
    "                else:\n",
    "                    row += 1\n",
    "    \n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelProcessor(DataProcessor):\n",
    "    \n",
    "    def get_train_examples(self, data_dir, skip_rows=0):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(data_dir, skip_rows), \"train\")\n",
    "    \n",
    "    def get_dev_examples(self, data_dir, skip_rows=0):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(data_dir, skip_rows), \"valid\")  \n",
    "    \n",
    "    def get_test_examples(self, data_dir, skip_rows=0):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(data_dir, skip_rows), \"test\")\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            label = eval(line[2])\n",
    "            text_a = line[1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "        tokens_b = None\n",
    "        \n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label = example.label\n",
    "        if ex_index < 5:\n",
    "            print(\"*** Example ***\")\n",
    "            print(\"guid: %s\" % (example.guid))\n",
    "            print(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            print(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            print(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            print(\"label: %s\" % (example.label))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label=label))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(btokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    # TODO: \"==\" deprecated\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up calculations on GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "gradient_accumulation_steps = 1 # Number of updates steps to accumulate before performing a backward/update pass.\n",
    "bert_config = \"bert-base-multilingual-cased\"\n",
    "data_dir = \"\"\n",
    "output_dir = \"\"\n",
    "max_seq_length = 60\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 8\n",
    "learning_rate = 5e-5 # initial learning rate for Adam\n",
    "num_train_epochs = 3\n",
    "warmup_proportion = 0.1 # for linear learning rate warmup\n",
    "num_labels = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMultiLabelClassification(PreTrainedBertModel):\n",
    "    \n",
    "    def __init__(self, config, num_labels=2):\n",
    "        \n",
    "        super(BertForMultiLabelClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.outputs = nn.Sigmoid()\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        # logits = self.outputs(logits)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss\n",
    "        else:\n",
    "            return self.outputs(logits)\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMultiLabelClassification.from_pretrained(bert_config, cache_dir= PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_-1', num_labels = num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "# layers with weight_decay i.e. regularization L2\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                    lr=learning_rate,\n",
    "                    warmup=warmup_proportion,\n",
    "                     t_total=num_train_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/sentences_reviews_train.csv', sep='|', header=None)\n",
    "df_train.to_csv('../data/sentences_reviews_train.tsv', sep='\\t')\n",
    "df_valid = pd.read_csv('../data/sentences_reviews_valid.csv', sep='|', header=None)\n",
    "df_valid.to_csv('../data/sentences_reviews_valid.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After all that, they complained to me about th...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  After all that, they complained to me about th...   \n",
       "\n",
       "                                         1  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "3  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>never swaying, never a bad meal, never bad ser...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm telling you...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you should travel from the Bronx to try it...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great food, great wine list, great service in ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>so delicious!!!!!!</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  never swaying, never a bad meal, never bad ser...   \n",
       "1                                 I'm telling you...   \n",
       "2      you should travel from the Bronx to try it...   \n",
       "3  great food, great wine list, great service in ...   \n",
       "4                                 so delicious!!!!!!   \n",
       "\n",
       "                                         1  \n",
       "0  [0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]  \n",
       "3  [0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0]  \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 65, 564, 108,  17,  40,  26, 306, 195,  25, 345,  54,  80, 214])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proportion of labels in train set\n",
    "np.stack(df_train[1].apply(eval).values).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 17, 117,  20,   3,   6,   4, 113,  31,   3,  76,  26,  17,  78])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proportion of labels in valid set\n",
    "np.stack(df_valid[1].apply(eval).values).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = MultiLabelProcessor()\n",
    "train_examples = processor.get_train_examples('../data/sentences_reviews_train.tsv', skip_rows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "guid: train-0\n",
      "tokens: [CLS] ju ##d ##ging from previous posts this used to be a good place , but not any longer . [SEP]\n",
      "input_ids: 101 23005 10162 13808 10188 16741 68699 10531 11031 10114 10347 169 15198 11192 117 10473 10472 11178 20165 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "*** Example ***\n",
      "guid: train-1\n",
      "tokens: [CLS] we , there were four of us , arrived at no ##on - the place was empty - and the staff acted like we were im ##posing on them and they were very rud ##e . [SEP]\n",
      "input_ids: 101 11951 117 11155 10309 11598 10108 19626 117 22584 10160 10192 10263 118 10105 11192 10134 65042 118 10111 10105 18927 48809 11850 11951 10309 10211 66735 10135 11345 10111 10689 10309 12558 101701 10112 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "*** Example ***\n",
      "guid: train-2\n",
      "tokens: [CLS] they never brought us com ##pli ##mentar ##y no ##od ##les , ig ##nore ##d repeated request ##s for sugar , and threw our dis ##hes on the table . [SEP]\n",
      "input_ids: 101 10689 14794 17327 19626 10212 62932 50923 10157 10192 12680 11268 117 23602 99772 10162 57026 37449 10107 10142 60390 117 10111 80516 17446 27920 19904 10135 10105 21783 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "*** Example ***\n",
      "guid: train-3\n",
      "tokens: [CLS] the food was lo ##us ##y - too sweet or too salt ##y and the portions tin ##y . [SEP]\n",
      "input_ids: 101 10105 18301 10134 10406 10251 10157 118 16683 72711 10345 16683 44253 10157 10111 10105 68902 21629 10157 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "*** Example ***\n",
      "guid: train-4\n",
      "tokens: [CLS] after all that , they com ##plained to me about the small tip . [SEP]\n",
      "input_ids: 101 10662 10435 10189 117 10689 10212 103154 10114 10911 10978 10105 12474 25119 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "featured_train_examples = convert_examples_to_features(train_examples, max_seq_length=60, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featured_train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_examples = processor.get_test_examples('../data/sentences_reviews_valid.tsv', skip_rows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "guid: test-0\n",
      "tokens: [CLS] never s ##way ##ing , never a bad me ##al , never bad service . . . [SEP]\n",
      "input_ids: 101 14794 187 14132 10230 117 14794 169 15838 10911 10415 117 14794 15838 11989 119 119 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: [0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
      "*** Example ***\n",
      "guid: test-1\n",
      "tokens: [CLS] i ' m telling you . . . [SEP]\n",
      "input_ids: 101 177 112 181 61758 13028 119 119 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "*** Example ***\n",
      "guid: test-2\n",
      "tokens: [CLS] you should travel from the bron ##x to try it . . . [SEP]\n",
      "input_ids: 101 13028 14819 23595 10188 10105 31806 10686 10114 31638 10271 119 119 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "*** Example ***\n",
      "guid: test-3\n",
      "tokens: [CLS] great food , great wine list , great service in a great neighborhood . . . [SEP]\n",
      "input_ids: 101 14772 18301 117 14772 43558 13416 117 14772 11989 10106 169 14772 37012 119 119 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: [0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0]\n",
      "*** Example ***\n",
      "guid: test-4\n",
      "tokens: [CLS] so del ##icio ##us ! ! ! ! ! ! [SEP]\n",
      "input_ids: 101 10380 10127 38036 10251 106 106 106 106 106 106 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "featured_valid_examples = convert_examples_to_features(valid_examples, max_seq_length=60, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors to be used in NN\n",
    "all_input_ids_train = torch.tensor([f.input_ids for f in featured_train_examples], dtype=torch.long)\n",
    "all_input_mask_train = torch.tensor([f.input_mask for f in featured_train_examples], dtype=torch.long)\n",
    "all_segment_ids_train = torch.tensor([f.segment_ids for f in featured_train_examples], dtype=torch.long)\n",
    "all_labels_train = torch.tensor([f.label for f in featured_train_examples], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(all_input_ids_train, all_input_mask_train, all_segment_ids_train, all_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids_valid = torch.tensor([f.input_ids for f in featured_valid_examples], dtype=torch.long)\n",
    "all_input_mask_valid = torch.tensor([f.input_mask for f in featured_valid_examples], dtype=torch.long)\n",
    "all_segment_ids_valid = torch.tensor([f.segment_ids for f in featured_valid_examples], dtype=torch.long)\n",
    "all_labels_valid = torch.tensor([f.label for f in featured_valid_examples], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = TensorDataset(all_input_ids_valid, all_input_mask_valid, all_segment_ids_valid, all_labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sampler = RandomSampler(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "num_train_steps = int(len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)\n",
    "print(num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = num_train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs[0]: tensor([0.5288, 0.5096, 0.5047, 0.4018, 0.4374, 0.4645, 0.5285, 0.5344, 0.4955,\n",
      "        0.5305, 0.4660, 0.5177, 0.4907], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "labels[0]: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|███▎      | 1/3 [00:43<01:27, 43.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 16.574214062690736\n",
      "outputs[0]: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "labels[0]: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|██████▋   | 2/3 [01:27<00:43, 43.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 38.99293050765991\n",
      "outputs[0]: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "labels[0]: tensor([0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 3/3 [02:11<00:00, 43.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 132.5465224456787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "\n",
    "for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "    \n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, labels = batch\n",
    "        \n",
    "        if step == 0:\n",
    "            outputs = model(input_ids, segment_ids, input_mask)\n",
    "            print(\"outputs[0]:\", outputs[0])\n",
    "            print(\"labels[0]:\", labels[0])\n",
    "        \n",
    "        loss = model(input_ids, segment_ids, input_mask, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # modify learning rate with special warm up BERT uses\n",
    "            lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n",
    "            lr_rates.append(lr_this_step)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "    print(\"Epoch:\", _, \"Loss:\", tr_loss / nb_tr_steps)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd5d1a0ab00>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a trained model\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model itself\n",
    "num_labels = model.num_labels\n",
    "output_model_file = os.path.join(output_dir, \"pytorch_multilabel.bin\")\n",
    "torch.save(model_to_save.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMultiLabelClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=13, bias=True)\n",
       "  (outputs): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a trained model that you have fine-tuned\n",
    "model_state_dict = torch.load(output_model_file)\n",
    "model = BertForMultiLabelClassification.from_pretrained(bert_config, state_dict=model_state_dict, num_labels=num_labels)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = TensorDataset(all_input_ids_valid, all_input_mask_valid, all_segment_ids_valid, all_labels_valid)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]/home/nicolas_mingione/anaconda3/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  after removing the cwd from sys.path.\n",
      "Iteration: 100%|██████████| 13/13 [00:06<00:00,  2.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Eval a model\n",
    "model = model.eval()\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "results = []\n",
    "inputs_ids_total = []\n",
    "\n",
    "for step, batch in enumerate(tqdm(valid_dataloader, desc=\"Iteration\")):\n",
    "        \n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, segment_ids, label_ids = batch\n",
    "    inputs_ids_total += [input_ids.numpy()]\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    \n",
    "    results += list(zip(logits, label_ids))\n",
    "    \n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2279, -0.2292, -0.2179,  ..., -0.2314, -0.2158,  0.2263],\n",
       "        [-0.2436, -0.2711, -0.2364,  ..., -0.2863, -0.2936,  0.2513],\n",
       "        [-0.1912, -0.2361, -0.1762,  ..., -0.2262, -0.2325,  0.2140],\n",
       "        ...,\n",
       "        [-0.2618, -0.2238, -0.2116,  ..., -0.2426, -0.2142,  0.2400],\n",
       "        [-0.2155, -0.2226, -0.2331,  ..., -0.2451, -0.2030,  0.1853],\n",
       "        [-0.2296, -0.2695, -0.2147,  ..., -0.2165, -0.2388,  0.2499]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['classifier.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy,\n",
    "          'global_step': global_step,\n",
    "          'loss': tr_loss/nb_tr_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153.24996713491586"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"eval_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_inputs = [x for z in inputs_ids_total for x in z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'help', '##ful', 'service', 'and', 'average', 'price', 'per', 'dis', '##h', '$', '10', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "(array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(lin_inputs[0]))\n",
    "print(results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
